{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "pA65CDBxZjPQ"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tensor Initialization"
      ],
      "metadata": {
        "id": "Bgtj-HgIaCzZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data = [[1, 2], [3, 4]]\n",
        "x_data = torch.tensor(data)"
      ],
      "metadata": {
        "id": "jLnbZ0u10ym2"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "np_array = np.array(data)\n",
        "x_np = torch.from_numpy(np_array)"
      ],
      "metadata": {
        "id": "2oRU9BTM00o6"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_ones = torch.ones_like(x_data) # retains the properties of x_data\n",
        "print(f\"Ones Tensor: \\n {x_ones} \\n\")\n",
        "\n",
        "x_rand = torch.rand_like(x_data, dtype=torch.float) # overrides the datatype of x_data\n",
        "print(f\"Random Tensor: \\n {x_rand} \\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xP4rvzeA02HQ",
        "outputId": "7c971912-42b6-4a64-fc78-28ad7df5d32b"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ones Tensor: \n",
            " tensor([[1, 1],\n",
            "        [1, 1]]) \n",
            "\n",
            "Random Tensor: \n",
            " tensor([[0.9259, 0.5115],\n",
            "        [0.7033, 0.3714]]) \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "shape = (2, 3,)\n",
        "rand_tensor = torch.rand(shape)\n",
        "ones_tensor = torch.ones(shape)\n",
        "zeros_tensor = torch.zeros(shape)\n",
        "\n",
        "print(f\"Random Tensor: \\n {rand_tensor} \\n\")\n",
        "print(f\"Ones Tensor: \\n {ones_tensor} \\n\")\n",
        "print(f\"Zeros Tensor: \\n {zeros_tensor}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QkQCGjGmbjFA",
        "outputId": "0ffd94ad-ef4a-4a63-f3a7-32ad34fcb211"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Random Tensor: \n",
            " tensor([[0.8835, 0.4686, 0.2455],\n",
            "        [0.9501, 0.5233, 0.5261]]) \n",
            "\n",
            "Ones Tensor: \n",
            " tensor([[1., 1., 1.],\n",
            "        [1., 1., 1.]]) \n",
            "\n",
            "Zeros Tensor: \n",
            " tensor([[0., 0., 0.],\n",
            "        [0., 0., 0.]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tensor Attributes"
      ],
      "metadata": {
        "id": "4lf90cuQbk9f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tensor = torch.rand(3, 4)\n",
        "print(f\"Shape of tensor: {tensor.shape}\")\n",
        "print(f\"Datatype of tensor: {tensor.dtype}\")\n",
        "print(f\"Device tensor is stored on: {tensor.device}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7JBX55p7blpj",
        "outputId": "c1d74e50-e6dd-41b5-f56f-cbd942027ebf"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of tensor: torch.Size([3, 4])\n",
            "Datatype of tensor: torch.float32\n",
            "Device tensor is stored on: cpu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tensor Operations"
      ],
      "metadata": {
        "id": "rxrNPqKCbow9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# We move our tensor to the GPU if available\n",
        "if torch.cuda.is_available():\n",
        "  tensor = tensor.to('cuda')\n",
        "  print(f\"Device tensor is stored on: {tensor.device}\")"
      ],
      "metadata": {
        "id": "8Bm5kV5Y1P2e"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tensor = torch.ones(4, 4)\n",
        "tensor[:,1] = 0\n",
        "print(tensor)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DdSx2A7o1ZVm",
        "outputId": "5b7cd348-d0f9-41df-afe0-aaae28c13a6e"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[1., 0., 1., 1.],\n",
            "        [1., 0., 1., 1.],\n",
            "        [1., 0., 1., 1.],\n",
            "        [1., 0., 1., 1.]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "t1 = torch.cat([tensor, tensor, tensor], dim=1)\n",
        "print(t1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eyfHCQa81bq5",
        "outputId": "001062eb-533a-4d88-c741-62ba728a39c5"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1.],\n",
            "        [1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1.],\n",
            "        [1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1.],\n",
            "        [1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1.]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# This computes the element-wise product\n",
        "print(f\"tensor.mul(tensor) \\n {tensor.mul(tensor)} \\n\")\n",
        "# Alternative syntax:\n",
        "print(f\"tensor * tensor \\n {tensor * tensor}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sw5HBs6P1btz",
        "outputId": "e1ad8e1d-8fde-4666-ddcc-cb9b70b4c75d"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor.mul(tensor) \n",
            " tensor([[1., 0., 1., 1.],\n",
            "        [1., 0., 1., 1.],\n",
            "        [1., 0., 1., 1.],\n",
            "        [1., 0., 1., 1.]]) \n",
            "\n",
            "tensor * tensor \n",
            " tensor([[1., 0., 1., 1.],\n",
            "        [1., 0., 1., 1.],\n",
            "        [1., 0., 1., 1.],\n",
            "        [1., 0., 1., 1.]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"tensor.matmul(tensor.T) \\n {tensor.matmul(tensor.T)} \\n\")\n",
        "# Alternative syntax:\n",
        "print(f\"tensor @ tensor.T \\n {tensor @ tensor.T}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fw2eWrVL1bxn",
        "outputId": "a0b88e0c-c6f7-45b8-9095-0d9c6a65cf5b"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor.matmul(tensor.T) \n",
            " tensor([[3., 3., 3., 3.],\n",
            "        [3., 3., 3., 3.],\n",
            "        [3., 3., 3., 3.],\n",
            "        [3., 3., 3., 3.]]) \n",
            "\n",
            "tensor @ tensor.T \n",
            " tensor([[3., 3., 3., 3.],\n",
            "        [3., 3., 3., 3.],\n",
            "        [3., 3., 3., 3.],\n",
            "        [3., 3., 3., 3.]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(tensor, \"\\n\")\n",
        "tensor.add_(5)\n",
        "print(tensor)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H__5qmwH1b0j",
        "outputId": "921f7aef-3569-4a31-dc93-3dc42e69d770"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[1., 0., 1., 1.],\n",
            "        [1., 0., 1., 1.],\n",
            "        [1., 0., 1., 1.],\n",
            "        [1., 0., 1., 1.]]) \n",
            "\n",
            "tensor([[6., 5., 6., 6.],\n",
            "        [6., 5., 6., 6.],\n",
            "        [6., 5., 6., 6.],\n",
            "        [6., 5., 6., 6.]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Bridge with NumPy"
      ],
      "metadata": {
        "id": "do9JK0Ju1l6M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "t = torch.ones(5)\n",
        "print(f\"t: {t}\")\n",
        "n = t.numpy()\n",
        "print(f\"n: {n}\")\n",
        "\n",
        "t.add_(1)\n",
        "print(f\"t: {t}\")\n",
        "print(f\"n: {n}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jaTZ5WgM1mqU",
        "outputId": "6faa0e25-1d09-4d24-886d-5dcb8f97fe8d"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "t: tensor([1., 1., 1., 1., 1.])\n",
            "n: [1. 1. 1. 1. 1.]\n",
            "t: tensor([2., 2., 2., 2., 2.])\n",
            "n: [2. 2. 2. 2. 2.]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "n = np.ones(5)\n",
        "t = torch.from_numpy(n)\n",
        "\n",
        "np.add(n, 1, out=n)\n",
        "print(f\"t: {t}\")\n",
        "print(f\"n: {n}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mjVZ6O8N1wEW",
        "outputId": "71149d10-140e-4879-e4f4-4d6469d12446"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "t: tensor([2., 2., 2., 2., 2.], dtype=torch.float64)\n",
            "n: [2. 2. 2. 2. 2.]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Differentiation in Autograd"
      ],
      "metadata": {
        "id": "iFiTLeX412o5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "a = torch.tensor([2., 3.], requires_grad=True)\n",
        "b = torch.tensor([6., 4.], requires_grad=True)"
      ],
      "metadata": {
        "id": "aOkFToY713aa"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Q = 3*a**3 - b**2"
      ],
      "metadata": {
        "id": "9-PH877j15xb"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "external_grad = torch.tensor([1., 1.])\n",
        "Q.backward(gradient=external_grad)"
      ],
      "metadata": {
        "id": "tfuwR_jl19tt"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# check if collected gradients are correct\n",
        "print(9*a**2 == a.grad)\n",
        "print(-2*b == b.grad)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jbd1UFcX2Cij",
        "outputId": "58d3bf63-a9d0-4d3c-f5ea-db62294f33b3"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([True, True])\n",
            "tensor([True, True])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Task 1"
      ],
      "metadata": {
        "id": "O_OE5wwi2KM7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "# point x1, x2 (1, 1)\n",
        "x1 = torch.tensor(1., requires_grad=True)\n",
        "x2 = torch.tensor(1., requires_grad=True)"
      ],
      "metadata": {
        "id": "Oq4CILZe5ImA"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Q = (3*x1 - 2*x2 - 2) ** 2"
      ],
      "metadata": {
        "id": "3w75QgFI5JKN"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "external_grad = torch.tensor(1.)\n",
        "Q.backward(gradient=external_grad)"
      ],
      "metadata": {
        "id": "S77K43ya5LJb"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# check if collected gradients are correct\n",
        "# Manually differenatiated\n",
        "dx1 = 2 * (3*x1 - 2*x2 - 2) * (3)\n",
        "dx2 = 2 * (3*x1 - 2*x2 - 2) * (-2)\n",
        "\n",
        "print(dx1 == x1.grad)\n",
        "print(dx2 == x2.grad)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NGyn1_o75LM5",
        "outputId": "caa29e08-5200-45af-ba6c-e4086b8d9b35"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(True)\n",
            "tensor(True)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Define the network"
      ],
      "metadata": {
        "id": "iI89zBKv62ee"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class Net(nn.Module):\n",
        "\n",
        "  def __init__(self):\n",
        "    super(Net, self).__init__()\n",
        "    # an affine operation: y = Wx + b\n",
        "    # 784 is the input dimension, and 68 is the output dimenstion of the first hidden layer\n",
        "    self.fc1 = nn.Linear(784, 64)\n",
        "    self.fc2 = nn.Linear(64, 64)\n",
        "    self.fc3 = nn.Linear(64, 10)\n",
        "\n",
        "  def forward(self, x):\n",
        "    # apply the first layer with relu activation\n",
        "    x = F.relu(self.fc1(x))\n",
        "    x = F.relu(self.fc2(x))\n",
        "    x = self.fc3(x)\n",
        "    return x\n",
        "\n",
        "net = Net()\n",
        "print(net)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TFMT_Uhb65dv",
        "outputId": "7e3c8a02-ea42-4515-abd6-bd2fdb9249e2"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Net(\n",
            "  (fc1): Linear(in_features=784, out_features=64, bias=True)\n",
            "  (fc2): Linear(in_features=64, out_features=64, bias=True)\n",
            "  (fc3): Linear(in_features=64, out_features=10, bias=True)\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "params = list(net.parameters())\n",
        "print(len(params))\n",
        "\n",
        "for p in params:\n",
        "  print(p.size())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w-3_5IYo7ZHX",
        "outputId": "fd433642-9efb-4841-c0cb-89e2ac35b855"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "6\n",
            "torch.Size([64, 784])\n",
            "torch.Size([64])\n",
            "torch.Size([64, 64])\n",
            "torch.Size([64])\n",
            "torch.Size([10, 64])\n",
            "torch.Size([10])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Task 2: Identify what are the parameters that are printed in the previous code"
      ],
      "metadata": {
        "id": "BaigovjO7dH8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "torch.Size([64, 784]): This represents the weight matrix for the first layer of the neural network. It has a size of 64 (output features) by 784 (input features).\n",
        "\n",
        "torch.Size([64]): This represents the bias vector for the first layer. It has a size of 64, corresponding to the number of output features in the first layer.\n",
        "\n",
        "torch.Size([64, 64]): This represents the weight matrix for the second layer of the neural network. It has a size of 64 (output features) by 64 (input features).\n",
        "\n",
        "torch.Size([64]): This represents the bias vector for the second layer. It has a size of 64, corresponding to the number of output features in the second layer.\n",
        "\n",
        "torch.Size([10, 64]): This represents the weight matrix for the third (output) layer of the neural network. It has a size of 10 (output classes) by 64 (input features from the previous layer).\n",
        "\n",
        "torch.Size([10]): This represents the bias vector for the third layer. It has a size of 10, corresponding to the number of output classes in the network."
      ],
      "metadata": {
        "id": "Z3-di0fk80Oe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "input = torch.randn(1, 784)\n",
        "out = net(input)\n",
        "\n",
        "print(out)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mM8hrnFM7pPM",
        "outputId": "0466d49d-00dd-448b-f441-9420646ac161"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[ 0.1856, -0.1204, -0.1846,  0.1195, -0.1009, -0.0451, -0.0875,  0.0976,\n",
            "          0.2077, -0.0421]], grad_fn=<AddmmBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Task 3: Try the previous network with a random mini-batch of size 4 and print its output."
      ],
      "metadata": {
        "id": "fJcyDW897pEC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "input = torch.randn(4, 784)\n",
        "out = net(input)\n",
        "\n",
        "print(out)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V9kQ4Rsa7083",
        "outputId": "d9d49265-f11d-4d76-acdc-1e916532d44b"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[ 0.2380, -0.2760,  0.0442,  0.2163, -0.2047,  0.1418, -0.0117,  0.1231,\n",
            "          0.3583,  0.0592],\n",
            "        [ 0.4374,  0.0455,  0.0613,  0.1743,  0.0275,  0.1085, -0.0276, -0.0471,\n",
            "          0.1526, -0.0157],\n",
            "        [ 0.1447, -0.1347, -0.0308,  0.0607, -0.1149, -0.0278, -0.0869,  0.0324,\n",
            "          0.1470, -0.0933],\n",
            "        [ 0.2013, -0.0901, -0.0048,  0.1984, -0.0526,  0.1496, -0.0522,  0.0393,\n",
            "          0.0365, -0.0606]], grad_fn=<AddmmBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Define a Loss function and optimizer"
      ],
      "metadata": {
        "id": "_EMckvxp78gH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.optim as optim\n",
        "\n",
        "loss = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(net.parameters(), lr=0.001)"
      ],
      "metadata": {
        "id": "hw6ndHlm79EV"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Loading a Dataset"
      ],
      "metadata": {
        "id": "vD6lgATDKzef"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "from torchvision import datasets\n",
        "from torchvision.transforms import ToTensor\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "training_data = datasets.MNIST(\n",
        "  root=\"data\",\n",
        "  train=True,\n",
        "  download=True,\n",
        "  transform=ToTensor()\n",
        ")\n",
        "\n",
        "test_data = datasets.MNIST(\n",
        "  root=\"data\",\n",
        "  train=False,\n",
        "  download=True,\n",
        "  transform=ToTensor()\n",
        ")"
      ],
      "metadata": {
        "id": "SBSMtqoC9I9j",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "288a38bd-acb6-4027-cd04-4ca6a267f904"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to data/MNIST/raw/train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 9912422/9912422 [00:00<00:00, 112064213.77it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting data/MNIST/raw/train-images-idx3-ubyte.gz to data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to data/MNIST/raw/train-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 28881/28881 [00:00<00:00, 25331596.37it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting data/MNIST/raw/train-labels-idx1-ubyte.gz to data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1648877/1648877 [00:00<00:00, 30496306.51it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting data/MNIST/raw/t10k-images-idx3-ubyte.gz to data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4542/4542 [00:00<00:00, 7415542.53it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting data/MNIST/raw/t10k-labels-idx1-ubyte.gz to data/MNIST/raw\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Iterating and Visualizing the Dataset"
      ],
      "metadata": {
        "id": "4gEcnSesKrGG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "figure = plt.figure(figsize=(8, 8))\n",
        "cols, rows = 3, 3\n",
        "\n",
        "for i in range(1, cols * rows + 1):\n",
        "  sample_idx = torch.randint(len(training_data), size=(1,)).item()\n",
        "  img, label = training_data[sample_idx]\n",
        "  figure.add_subplot(rows, cols, i)\n",
        "  plt.title(\"digit:\" + str(label))\n",
        "  plt.axis(\"off\")\n",
        "  plt.imshow(img.squeeze(), cmap=\"gray\")\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 675
        },
        "id": "_zhhbznC9P5i",
        "outputId": "d2008895-f475-4744-b5b2-6565fcc1b4b6"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 800x800 with 9 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Preparing your data for training with DataLoaders"
      ],
      "metadata": {
        "id": "sc0XMO-GK3ZY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "train_dataloader = DataLoader(training_data, batch_size=4, shuffle=True)\n",
        "test_dataloader = DataLoader(test_data, batch_size=4, shuffle=True)"
      ],
      "metadata": {
        "id": "SsR1m9Pl9YK1"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Iterate through the DataLoader"
      ],
      "metadata": {
        "id": "RHJsmIi2K7J2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Display image and label.\n",
        "train_features, train_labels = next(iter(train_dataloader))\n",
        "print(f\"Feature batch shape: {train_features.size()}\")\n",
        "print(f\"Labels batch shape: {train_labels.size()}\")\n",
        "img = train_features[0].squeeze()\n",
        "label = train_labels[0]\n",
        "plt.imshow(img, cmap=\"gray\")\n",
        "plt.show()\n",
        "print(f\"Label: {label}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 485
        },
        "id": "w8Y3LLC79ZsC",
        "outputId": "6ebd84a6-60e0-4280-c0f0-4dbb307c881d"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Feature batch shape: torch.Size([4, 1, 28, 28])\n",
            "Labels batch shape: torch.Size([4])\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Label: 5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train the network"
      ],
      "metadata": {
        "id": "zO9BxIBkK9Ni"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(2): # loop over the dataset multiple times\n",
        "\n",
        "  running_loss = 0.0\n",
        "  for i, data in enumerate(train_dataloader, 0):\n",
        "    # get the inputs; data is a list of [inputs, labels]\n",
        "    inputs, labels = data\n",
        "    # zero the parameter gradients\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    # forward + backward + optimize\n",
        "    outputs = net(torch.flatten(inputs,1))\n",
        "    iteration_loss = loss(outputs, labels)\n",
        "    iteration_loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    # print statistics\n",
        "    running_loss += iteration_loss.item()\n",
        "    if i % 2000 == 1999: # print every 2000 mini-batches\n",
        "      print(f'[{epoch + 1}, {i + 1:5d}] loss: {running_loss / 2000:.3f}')\n",
        "      running_loss = 0.0\n",
        "\n",
        "print('Finished Training')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9Ft-mnab9kGS",
        "outputId": "0506f386-8c8a-414e-84c7-6621192aa289"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1,  2000] loss: 2.285\n",
            "[1,  4000] loss: 2.198\n",
            "[1,  6000] loss: 1.970\n",
            "[1,  8000] loss: 1.510\n",
            "[1, 10000] loss: 0.997\n",
            "[1, 12000] loss: 0.762\n",
            "[1, 14000] loss: 0.623\n",
            "[2,  2000] loss: 0.538\n",
            "[2,  4000] loss: 0.460\n",
            "[2,  6000] loss: 0.430\n",
            "[2,  8000] loss: 0.420\n",
            "[2, 10000] loss: 0.400\n",
            "[2, 12000] loss: 0.406\n",
            "[2, 14000] loss: 0.389\n",
            "Finished Training\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Task 4: What is the meaning of epoch, forward pass, backward pass. What is the effect of torch.flatten(inputs, 1), and optimizer.step()?"
      ],
      "metadata": {
        "id": "udM_XNUNJ561"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Epoch: An epoch is a single pass through the entire training dataset. The outer loop for epoch in range(2) indicates that the training process will loop over the dataset two times.\n",
        "\n",
        "Forward Pass: The forward pass is the process of passing input data through the neural network to compute the predicted outputs. In this code, outputs = net(torch.flatten(inputs, 1)) represents the forward pass, where inputs are passed through the neural network (net) after being flattened along the second dimension using torch.flatten(inputs, 1).\n",
        "\n",
        "Backward Pass: The backward pass, also known as backpropagation, is the process of computing gradients of the loss function with respect to the network's parameters. This is done using the backward() method: iteration_loss.backward(). It computes gradients for all the tensors used to compute iteration_loss.\n",
        "\n",
        "Optimizer: optimizer.step() updates the parameters of the neural network using the computed gradients and the optimization algorithm (e.g., SGD, Adam) to minimize the loss. It adjusts the network's weights and biases based on the computed gradients and the specified optimization strategy.\n",
        "\n",
        "torch.flatten(inputs, 1): This function reshapes the input tensor inputs to have a flattened shape along the second dimension (dimension 1). It's likely used to prepare the input data for the neural network if it expects a flattened representation."
      ],
      "metadata": {
        "id": "PSJygMBNJ59p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "PATH = './my_net.pth'\n",
        "torch.save(net.state_dict(), PATH)"
      ],
      "metadata": {
        "id": "edqjMAUyJ_1M"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Test the network on the test data"
      ],
      "metadata": {
        "id": "3bNr6fNpLBgj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "net = Net()\n",
        "net.load_state_dict(torch.load(PATH))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mUYTvktsKAt9",
        "outputId": "cbfab778-3a53-4b82-c44c-2e1cc024dd3a"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "correct = 0\n",
        "total = 0\n",
        "# since we're not training, we don't need to calculate the gradients for our outputs\n",
        "with torch.no_grad():\n",
        "  for data in test_dataloader:\n",
        "    images, labels = data\n",
        "    # calculate outputs by running images through the network\n",
        "    outputs = net(torch.flatten(images,1))\n",
        "    # the class with the highest energy is what we choose as prediction\n",
        "    _, predicted = torch.max(outputs.data, 1)\n",
        "    total += labels.size(0)\n",
        "    correct += (predicted == labels).sum().item()\n",
        "print(f'Accuracy of the network on the 10000 test images: {100 * correct// total} %')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TAO8dtv4KDbZ",
        "outputId": "1866dec4-516b-4a3b-d902-36725d6d515f"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of the network on the 10000 test images: 89 %\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Task 5: Train the network in the previous example, but instead of using 2 hidden layers, try 3 hidden layers."
      ],
      "metadata": {
        "id": "rNHGyEwjKSPK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# Assuming net is the previously defined neural network with 2 hidden layers\n",
        "\n",
        "class ThreeHiddenLayerNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(ThreeHiddenLayerNet, self).__init__()\n",
        "        self.fc1 = nn.Linear(784, 64)  # First hidden layer\n",
        "        self.fc2 = nn.Linear(64, 64)   # Second hidden layer\n",
        "        self.fc3 = nn.Linear(64, 64)   # Third hidden layer\n",
        "        self.fc4 = nn.Linear(64, 10)   # Output layer\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.flatten(x, 1)\n",
        "        x = torch.relu(self.fc1(x))   # ReLU activation for first hidden layer\n",
        "        x = torch.relu(self.fc2(x))   # ReLU activation for second hidden layer\n",
        "        x = torch.relu(self.fc3(x))   # ReLU activation for third hidden layer\n",
        "        x = self.fc4(x)               # Output layer without activation (for example, using CrossEntropyLoss)\n",
        "        return x"
      ],
      "metadata": {
        "id": "-g107n19Q2Je"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the network\n",
        "net = ThreeHiddenLayerNet()\n",
        "\n",
        "# Define the loss function and optimizer\n",
        "loss = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(net.parameters(), lr=0.001)\n",
        "\n",
        "for epoch in range(2): # loop over the dataset multiple times\n",
        "\n",
        "  running_loss = 0.0\n",
        "  for i, data in enumerate(train_dataloader, 0):\n",
        "    # get the inputs; data is a list of [inputs, labels]\n",
        "    inputs, labels = data\n",
        "    # zero the parameter gradients\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    # forward + backward + optimize\n",
        "    outputs = net(torch.flatten(inputs,1))\n",
        "    iteration_loss = loss(outputs, labels)\n",
        "    iteration_loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    # print statistics\n",
        "    running_loss += iteration_loss.item()\n",
        "    if i % 2000 == 1999: # print every 2000 mini-batches\n",
        "      print(f'[{epoch + 1}, {i + 1:5d}] loss: {running_loss / 2000:.3f}')\n",
        "      running_loss = 0.0\n",
        "\n",
        "print('Finished Training')\n",
        "\n",
        "correct = 0\n",
        "total = 0\n",
        "# since we're not training, we don't need to calculate the gradients for our outputs\n",
        "with torch.no_grad():\n",
        "  for data in test_dataloader:\n",
        "    images, labels = data\n",
        "    # calculate outputs by running images through the network\n",
        "    outputs = net(torch.flatten(images,1))\n",
        "    # the class with the highest energy is what we choose as prediction\n",
        "    _, predicted = torch.max(outputs.data, 1)\n",
        "    total += labels.size(0)\n",
        "    correct += (predicted == labels).sum().item()\n",
        "\n",
        "print(f'Accuracy of the network on the 10000 test images: {100 * correct// total} %')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CuuNNRQzqrcK",
        "outputId": "931e2a6f-a786-428c-ee7b-a379cf6650dd"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1,  2000] loss: 2.299\n",
            "[1,  4000] loss: 2.286\n",
            "[1,  6000] loss: 2.262\n",
            "[1,  8000] loss: 2.205\n",
            "[1, 10000] loss: 2.039\n",
            "[1, 12000] loss: 1.679\n",
            "[1, 14000] loss: 1.232\n",
            "[2,  2000] loss: 0.833\n",
            "[2,  4000] loss: 0.723\n",
            "[2,  6000] loss: 0.638\n",
            "[2,  8000] loss: 0.582\n",
            "[2, 10000] loss: 0.567\n",
            "[2, 12000] loss: 0.522\n",
            "[2, 14000] loss: 0.483\n",
            "Finished Training\n",
            "Accuracy of the network on the 10000 test images: 87 %\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Task 6: Train the network in the previous example using Adam optimizer"
      ],
      "metadata": {
        "id": "EgSQEc66KTI3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the network\n",
        "net = ThreeHiddenLayerNet()\n",
        "\n",
        "# Use Adam optimizer with a learning rate of 0.001\n",
        "loss = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(net.parameters(), lr=0.001)\n",
        "\n",
        "# Assuming train_dataloader contains your training dataset\n",
        "for epoch in range(2):  # loop over the dataset multiple times\n",
        "    running_loss = 0.0\n",
        "    for i, data in enumerate(train_dataloader, 0):\n",
        "        inputs, labels = data\n",
        "        optimizer.zero_grad()\n",
        "        outputs = net(inputs)\n",
        "        iteration_loss = loss(outputs, labels)\n",
        "        iteration_loss.backward()\n",
        "        optimizer.step()\n",
        "        running_loss += iteration_loss.item()\n",
        "        if i % 2000 == 1999:  # print every 2000 mini-batches\n",
        "            print(f'[{epoch + 1}, {i + 1:5d}] loss: {running_loss / 2000:.3f}')\n",
        "            running_loss = 0.0\n",
        "\n",
        "print('Finished Training')\n",
        "\n",
        "correct = 0\n",
        "total = 0\n",
        "# since we're not training, we don't need to calculate the gradients for our outputs\n",
        "with torch.no_grad():\n",
        "  for data in test_dataloader:\n",
        "    images, labels = data\n",
        "    # calculate outputs by running images through the network\n",
        "    outputs = net(torch.flatten(images,1))\n",
        "    # the class with the highest energy is what we choose as prediction\n",
        "    _, predicted = torch.max(outputs.data, 1)\n",
        "    total += labels.size(0)\n",
        "    correct += (predicted == labels).sum().item()\n",
        "\n",
        "print(f'Accuracy of the network on the 10000 test images: {100 * correct// total} %')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VTBAqk5uKUpI",
        "outputId": "bb8d0a3f-e572-41b7-9310-89a377c0332f"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1,  2000] loss: 0.571\n",
            "[1,  4000] loss: 0.295\n",
            "[1,  6000] loss: 0.232\n",
            "[1,  8000] loss: 0.214\n",
            "[1, 10000] loss: 0.190\n",
            "[1, 12000] loss: 0.184\n",
            "[1, 14000] loss: 0.168\n",
            "[2,  2000] loss: 0.128\n",
            "[2,  4000] loss: 0.137\n",
            "[2,  6000] loss: 0.141\n",
            "[2,  8000] loss: 0.140\n",
            "[2, 10000] loss: 0.132\n",
            "[2, 12000] loss: 0.134\n",
            "[2, 14000] loss: 0.116\n",
            "Finished Training\n",
            "Accuracy of the network on the 10000 test images: 96 %\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training on GPU"
      ],
      "metadata": {
        "id": "Pjxa4_pvLFgW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Assuming that we are on a CUDA machine, this should print a CUDA device:\n",
        "print(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XXl9R15CKWlp",
        "outputId": "0d67bf4e-1472-49ba-db83-ad6676a639ec"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cpu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "net = Net()\n",
        "\n",
        "net.to(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KTRdWzO7KZFK",
        "outputId": "46334019-e30e-47bf-bf0f-ede4b3b658d7"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Net(\n",
              "  (fc1): Linear(in_features=784, out_features=64, bias=True)\n",
              "  (fc2): Linear(in_features=64, out_features=64, bias=True)\n",
              "  (fc3): Linear(in_features=64, out_features=10, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Task 7: Train the network in the previous example on GPU. Do you notice significant speedup? if not, try to increase the size of your network."
      ],
      "metadata": {
        "id": "s6v1S2VYKdXV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "First, we try on small network."
      ],
      "metadata": {
        "id": "TLqnx02fwbDJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "loss = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(net.parameters(), lr=0.001)\n",
        "\n",
        "for epoch in range(2): # loop over the dataset multiple times\n",
        "\n",
        "  running_loss = 0.0\n",
        "  for i, data in enumerate(train_dataloader, 0):\n",
        "    # get the inputs; data is a list of [inputs, labels]\n",
        "    inputs, labels = data[0].to(device), data[1].to(device)\n",
        "    # zero the parameter gradients\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    # forward + backward + optimize\n",
        "    outputs = net(torch.flatten(inputs,1))\n",
        "    iteration_loss = loss(outputs, labels)\n",
        "    iteration_loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    # print statistics\n",
        "    running_loss += iteration_loss.item()\n",
        "    if i % 2000 == 1999: # print every 2000 mini-batches\n",
        "      print(f'[{epoch + 1}, {i + 1:5d}] loss: {running_loss / 2000:.3f}')\n",
        "      running_loss = 0.0\n",
        "\n",
        "print('Finished Training')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lmqLlejeKbEh",
        "outputId": "f5baff4e-a3d7-4f94-de6c-75ca17b49a45"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1,  2000] loss: 0.068\n",
            "[1,  4000] loss: 0.056\n",
            "[1,  6000] loss: 0.060\n",
            "[1,  8000] loss: 0.056\n",
            "[1, 10000] loss: 0.050\n",
            "[1, 12000] loss: 0.057\n",
            "[1, 14000] loss: 0.048\n",
            "[2,  2000] loss: 0.042\n",
            "[2,  4000] loss: 0.041\n",
            "[2,  6000] loss: 0.050\n",
            "[2,  8000] loss: 0.040\n",
            "[2, 10000] loss: 0.045\n",
            "[2, 12000] loss: 0.044\n",
            "[2, 14000] loss: 0.042\n",
            "Finished Training\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "CPU time: 46s\n",
        "\n",
        "GPU time: 1m\n",
        "\n",
        "We can't see the speedup, even CPU is faster. So, we will increase the size of the network as follows:"
      ],
      "metadata": {
        "id": "ytXAdevch2ow"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class LargerThreeHiddenLayerNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(LargerThreeHiddenLayerNet, self).__init__()\n",
        "        self.fc1 = nn.Linear(784, 512)  # First hidden layer with 128 neurons\n",
        "        self.fc2 = nn.Linear(512, 512)  # Second hidden layer with 128 neurons\n",
        "        self.fc3 = nn.Linear(512, 10)   # Output layer with 10 neurons\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.flatten(x, 1)\n",
        "        x = torch.relu(self.fc1(x))   # ReLU activation for first hidden layer\n",
        "        x = torch.relu(self.fc2(x))   # ReLU activation for second hidden layer\n",
        "        x = self.fc3(x)               # Output layer without activation (for example, using CrossEntropyLoss)\n",
        "        return x"
      ],
      "metadata": {
        "id": "nXfcNnASwvAU"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# Assuming net is the previously defined neural network with 3 hidden layers\n",
        "\n",
        "# Check if GPU is available and set the device\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Training on {device}\")\n",
        "\n",
        "# Initialize the larger network and move it to the device (GPU if available)\n",
        "net = LargerThreeHiddenLayerNet().to(device)\n",
        "\n",
        "# Define the loss function\n",
        "loss = nn.CrossEntropyLoss()\n",
        "\n",
        "# Use Adam optimizer with a learning rate of 0.001\n",
        "optimizer = optim.Adam(net.parameters(), lr=0.001)\n",
        "\n",
        "# Assuming train_dataloader contains your training dataset\n",
        "for epoch in range(2):  # loop over the dataset multiple times\n",
        "    running_loss = 0.0\n",
        "    for i, data in enumerate(train_dataloader, 0):\n",
        "        inputs, labels = data\n",
        "        # Move the inputs and labels to the device\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = net(inputs)\n",
        "        iteration_loss = loss(outputs, labels)\n",
        "        iteration_loss.backward()\n",
        "        optimizer.step()\n",
        "        running_loss += iteration_loss.item()\n",
        "        if i % 2000 == 1999:  # print every 2000 mini-batches\n",
        "            print(f'[{epoch + 1}, {i + 1:5d}] loss: {running_loss / 2000:.3f}')\n",
        "            running_loss = 0.0\n",
        "\n",
        "print('Finished Training')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O6vOusnXmE0s",
        "outputId": "3fe49957-9df3-4e40-e1f7-ea7f10904e80"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training on cpu\n",
            "[1,  2000] loss: 0.423\n",
            "[1,  4000] loss: 0.243\n",
            "[1,  6000] loss: 0.214\n",
            "[1,  8000] loss: 0.180\n",
            "[1, 10000] loss: 0.178\n",
            "[1, 12000] loss: 0.146\n",
            "[1, 14000] loss: 0.156\n",
            "[2,  2000] loss: 0.104\n",
            "[2,  4000] loss: 0.128\n",
            "[2,  6000] loss: 0.124\n",
            "[2,  8000] loss: 0.126\n",
            "[2, 10000] loss: 0.121\n",
            "[2, 12000] loss: 0.114\n",
            "[2, 14000] loss: 0.116\n",
            "Finished Training\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# Assuming net is the previously defined neural network with 3 hidden layers\n",
        "\n",
        "# Check if GPU is available and set the device\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Training on {device}\")\n",
        "\n",
        "# Initialize the larger network and move it to the device (GPU if available)\n",
        "net = LargerThreeHiddenLayerNet().to(device)\n",
        "\n",
        "# Define the loss function\n",
        "loss = nn.CrossEntropyLoss()\n",
        "\n",
        "# Use Adam optimizer with a learning rate of 0.001\n",
        "optimizer = optim.Adam(net.parameters(), lr=0.001)\n",
        "\n",
        "# Assuming train_dataloader contains your training dataset\n",
        "for epoch in range(2):  # loop over the dataset multiple times\n",
        "    running_loss = 0.0\n",
        "    for i, data in enumerate(train_dataloader, 0):\n",
        "        inputs, labels = data\n",
        "        # Move the inputs and labels to the device\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = net(inputs)\n",
        "        iteration_loss = loss(outputs, labels)\n",
        "        iteration_loss.backward()\n",
        "        optimizer.step()\n",
        "        running_loss += iteration_loss.item()\n",
        "        if i % 2000 == 1999:  # print every 2000 mini-batches\n",
        "            print(f'[{epoch + 1}, {i + 1:5d}] loss: {running_loss / 2000:.3f}')\n",
        "            running_loss = 0.0\n",
        "\n",
        "print('Finished Training')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qwGlLoW7vhyx",
        "outputId": "551a8904-7bfd-4a3c-acfc-a57a0b30f225"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training on cuda:0\n",
            "[1,  2000] loss: 0.409\n",
            "[1,  4000] loss: 0.230\n",
            "[1,  6000] loss: 0.191\n",
            "[1,  8000] loss: 0.184\n",
            "[1, 10000] loss: 0.179\n",
            "[1, 12000] loss: 0.159\n",
            "[1, 14000] loss: 0.152\n",
            "[2,  2000] loss: 0.120\n",
            "[2,  4000] loss: 0.116\n",
            "[2,  6000] loss: 0.124\n",
            "[2,  8000] loss: 0.127\n",
            "[2, 10000] loss: 0.111\n",
            "[2, 12000] loss: 0.120\n",
            "[2, 14000] loss: 0.110\n",
            "Finished Training\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "GPU time: 1m 19s\n",
        "\n",
        "CPU time: 9m 4s\n",
        "\n",
        "Now, we can see the huge difference between the run on CPU and GPU"
      ],
      "metadata": {
        "id": "91PK3UwYtYEU"
      }
    }
  ]
}